type: plm
name: bert_biaffine
delimiter: "\t"
input: "resources/data/dataset/ner/zh/ccks/address/0621"
file_format: biaffine
output: "resources/data/output/ner/zh/ccks/address/0621/bert_biaffine/nezha-base-chinese/ce"
summary: "resources/data/output/ner/zh/ccks/address/0621/bert_biaffine/nezha-base-chinese/ce/summary"
max_length: 60
model_path: resources/data/pretrained/nezha-base-chinese
per_device_train_batch_size: 16
per_device_eval_batch_size: 8
plm_lr: 1.0297127640588862e-05
not_plm_lr: 5.143696885811861e-05
weight_decay: 0.0
model_type: bert
decode_type: biaffine
loss_name: "ce"
num_train_epochs: 14
gradient_accumulation_steps: 1
lr_scheduler_type: linear
num_warmup_steps: 0
seed: 100
do_lower_case: True
task_name: ner
cpu: False

# seed:100, plm_lr:2.0228447859367986e-5, not_plm_lr:7.881173748974317e-5, epoch:15
# seed:31, plm_lr:3.703369460189865e-5, not:0.0007768910276375454, epoch:16, value:0.945573
# ce: Trial 12 finished with value: 0.9477646573505905 and parameters: {'seed': 100, 'plm_lr': 1.0297127640588862e-05, 'not_plm_lr': 5.143696885811861e-05, 'num_train_epochs': 14}. Best is trial 12 with value: 0.9477646573505905
# focal:
#   Best trial:
#   Value:  0.9452422770947412
#   Params: 
#     seed: 31
#     plm_lr: 3.113790515152554e-05
#     not_plm_lr: 0.0001387823250664349
#     num_train_epochs: 10